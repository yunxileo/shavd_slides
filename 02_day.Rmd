---
title: "Tree model"
output: 
  html_notebook: 
    toc: yes
    toc_depth: 5
---

## decision tree

### ID3算法

#### 信息论

+ 熵与信息熵

熵是描述事物无序性的参数，熵越大则无序性越强,既熵度量了事物的不确定性，越不确定的事物，它的熵就越大。信息熵公式定义：

设一次随机事件（用随机变量X表示），它可能会$x_1, x_2, x_3, \cdots ,x_m$有共m个不同的结果，每个结果出现的概率分别为$p_1, p_2, p_3, \cdots, p_m$
，那么XX的不确定度，即信息熵为：
$$
H(X) =\sum_{i=1}^{m} p_i \cdot \log_{2} \frac{1}{p_i} = - \sum_{i=1}^{m} p_i \cdot \log_{2} p_i         
$$

+ 联合熵

熟悉了一个变量X的熵，很容易推广到多个个变量的联合熵，这里给出两个变量X和Y的联合熵表达式：
$$H(X,Y) = -\sum\limits_{i=1}^{m}p(x_i,y_i)logp(x_i,y_i)$$

+ 条件熵 

设X,Y为两个随机变量，在X发生的前提下，Y发生所新带来的熵,定义为Y的条件熵（Conditional Entropy),用H(Y|X)表示，计算公式如下：

$$
H(Y|X) = - \sum_{x_i,y_j}^{m,n} p(x_i,y_j) \cdot log_2 p(y_j|x_i) 
$$
其物理含义是当变量X已知时，变量Y的平均不确定性是多少。

假设变量X取值有m个，那么H(Y|X=xi)H(Y|X=xi)是指变量XX被固定为值xi时的条件熵；H(Y|X)H(Y|X)时指变量X被固定时的条件熵。那么二者之间的关系时：

$$

\begin{align} H(Y|X) & = p(x_1) \cdot H(Y|X=x_1) +  \cdots + p(x_m) \cdot H(Y|X=x_m) \\ & = \sum_{i=1}^{m} p(x_i) \cdot H(Y|X=x_i) \end{align}  
$$

$$
\begin{align} H(Y|X) & = \sum_{i=1}^{m} p(x_i) \cdot H(Y|X=x_i) \\ & = -\sum_{i=1}^{m} p(x_i) \cdot \left( \sum_{j=i}^{n} p(y_j|x_i) \cdot log_2 p(y_j|x_i) \right) \\ & = -\sum_{i=1}^{m} \sum_{j=1}^{n} p(y_j,x_i) \cdot log_2 p(y_j|x_i) \\ & = - \sum_{x_i,y_j}^{m,n} p(x_i,y_j) \cdot log_2 p(y_j|x_i) \end{align}    
$$

+ 互信息与信息增益 

互信息就是随机事件XX的不确定性（即熵H(X)），以及在给定随机变量Y条件下的不确定性（即条件熵H(X|Y)）之间的差异，即

$$
I(X;Y) = H(X) - H(X|Y) 
$$
在决策树中，互信息也叫作信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。


#### 算法思路 

输入的是m个样本，样本输出集合为D，每个样本有n个离散特征，特征集合即为A，输出为决策树T。
算法的过程为：


1) 初始化信息增益的阈值ϵ


2) 判断样本是否为同一类输出DiDi，如果是则返回单节点树T。标记类别为DiDi


3) 判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。
　　　
　　　
4) 计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征Ag
　　
　　　
5) 如果Ag的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。


6) 否则，按特征Ag的不同取值Ag将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为Ag。返回增加了节点的数T。


7)对于所有的子节点，令$D=D_i,  A= A-\{A_g\}$递归调用2-6步，得到子树TiTi并返回。

#### 算法不足 
ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。　　


a)ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。


b)ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？


c) ID3算法对于缺失值的情况没有做考虑


d) 没有考虑过拟合的问题


ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法，也许你会问，为什么不叫ID4，ID5之类的名字呢?那是因为决策树太火爆，他的ID3一出来，别人二次创新，很快 就占了ID4， ID5，所以他另辟蹊径，取名C4.0算法，后来的进化版为C4.5算法。下面我们就来聊下C4.5算法



### C4.5

上一节我们讲到ID3算法有四个主要的不足，C4.5算法是用于生成决策树的一种经典算法，是ID3算法的一种延伸和优化。C4.5算法对ID3算法主要做了一下几点改进： 

1 通过信息增益率选择分裂属性，克服了ID3算法中通过信息增益倾向于选择拥有多个属性值的属性作为分裂属性的不足； 

2 能够处理离散型和连续型的属性类型，即将连续型的属性进行离散化处理； 


3 能够处理具有缺失属性值的训练数据；


4 构造决策树之后进行剪枝操作； 


#### 信息增益率

分裂属性选择的评判标准是决策树算法之间的根本区别。区别于ID3算法通过信息增益选择分裂属性，C4.5算法通过信息增益率选择分裂属性.

特征A对训练集D的信息增益率 $I_R(D,A)$定义为其信息增益$I(A,D)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即
$$I_R(D,A) = \frac{I(A,D)}{H_A(D)}$$
其中：
$H_A(D) = -\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$,n为特征A的类别数。
特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。

#### 连续值处理

#### 缺失值处理

####  剪枝

#### 缺点与不足

1)由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。后面在下篇讲CART树的时候我们会专门讲决策树的减枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。


2)C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。


3)C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。


4)C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。

### CART

#### 基尼指数 

我们知道，在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。

具体的，在分类问题中，假设有K个类别，第k个类别的概率为pk, 则基尼系数的表达式为：

$$ 
Gini(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2
$$


对于个给定的样本D,假设有K个类别, 第k个类别的数量为CkCk,则样本D的基尼系数表达式为：

$$
Gini(D) = 1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$
特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：

$$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)$$

基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经过A=a分割之后集合D的不确定性。基尼指数越大，样本集合的不确定性就越大，这一点与熵相似。 

#### 算法过程 

算法输入是训练集D，基尼系数的阈值，样本个数阈值。

输出是决策树T。

我们的算法从根节点开始，用训练集递归的建立CART树。


1) 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。


2) 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。


3) 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见第二节。缺失值的处理方法和上篇的C4.5算法里描述的相同。


4) 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.


5) 对左右的子节点递归的调用1-4步，生成决策树。



#### CART剪枝 

就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。

一 剪枝，形成一个子树序列 


首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：

$$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t| $$

其中，αα为正则化参数，这和线性回归的正则化一样。$C(T_t)$为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。$T_t$是子树T的叶子节点的数量。


当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当$\alpha = \infty$时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的αα，一定存在使损失函数$C_{\alpha}(T)$最小的唯一子树。


看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树TtTt，如果没有剪枝，它的损失是

$$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|$$ 

如果将其剪掉，仅仅保留根节点，则损失是:

$$C_{\alpha}(T) = C(T) + \alpha$$
当α=0或者α很小时，$C_{\alpha}(T_t) < C_{\alpha}(T)$ , 当α增大到一定的程度时$C_{\alpha}(T_t) = C_{\alpha}(T)$

当α继续增大时不等式反向，也就是说，如果满足下式：
$\alpha = \frac{C(T)-C(T_t)}{|T_t|-1}$

　Tt和T有相同的损失函数，但是T节点更少，因此可以对子树Tt进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T。

最后我们看看CART树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的αα所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。


二 在剪枝的子树序列中，通过交叉验证选取最优子树 


具体的利用独立的验证数据集，测试各子树序列中各课子树的平均误差或者基尼指数，平方误差或者基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每颗子树都对应一个$\alpha$，所以当最优的子树$T_k$确定时，最优的$\alpha_k$也就确定了，即得到了最优的决策树

#### 剪枝算法 

输入：CART树建立算法得到的原始决策树$T_0$。

输出：最优决策子树$T_\alpha$。

(1) 初始化$T = T_\alpha,\alpha = + \infty$


(2) 从叶子节点开始自下而上计算各内部节点t的训练误差损失函数$C_{\alpha}(T_t)$(回归树为均方差，分类树为基尼系数), 叶子节点数|Tt|，以及正则化阈值$\alpha= min\{\frac{C(T)-C(T_t)}{|T_t|-1}, \alpha_{min}\}$, 更新$\alpha_{min}= \alpha$ ,


(3)得到所有节点的α值的集合M。

(4)从M中选择最大的值$\alpha_k$，自上而下的访问子树t的内部节点，如果$\frac{C(T)-C(T_t)}{|T_t|-1} \leq \alpha_k$时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到$\alpha_k$对应的最优子树$T_k$


(5)最优子树集合$\omega=\omega \cup T_k$ ,$M= M -\{\alpha_k\}$


(6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合ω.


(7) 采用交叉验证在ω选择最优子树$T_\alpha$

### 决策树算法小结 

#### 优点：

　　　　1）简单直观，生成的决策树很直观。

　　　　2）基本不需要预处理，不需要提前归一化，处理缺失值。

　　　　3）使用决策树预测的代价是$O(log_2m)$。 m为样本数。

　　　　4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。

　　　　5）可以处理多维度输出的分类问题。

　　　　6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释

　　　　7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。

　　　　8） 对于异常点的容错能力好，健壮性高。

#### 缺点:

　　　　1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。

　　　　2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。

　　　　3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。

　　　　4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。

　　　　5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。



##  random forest


















## adaboost


## boosting


## xgboost


## lightgbm 


## 参考文献

1 [http://www.cnblogs.com/pinard/p/6050306.html] (决策树原理)

2 [https://en.wikipedia.org/wiki/Conditional_entropy] (条件熵)

3 [https://www.zhihu.com/question/22697086] (CART剪枝)
